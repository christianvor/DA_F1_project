---
title: "Predicting_Finishing_Position"
author: "Wiktor"
format: html
editor: visual
---

```{r}
library(ggplot2)
library(tidyverse)
```

The examination of the topic was started by taking the pre-processed data and checking it's structure as well as its summary. This was done in order to better understand the data set and to formulate a research problem.

```{r}
data <- read.csv('data/F1_preprocessed')

head(data)
str(data)
summary(data)
```

The data was cleaned to eliminate a problem by including the entries with NAs or with the finishing position at 24. The finishing position of 24 is the last possible position and in the pre-processing step all of the drivers that did not finish the race were also assigned a finishing position of 24. The elimination of all entries with this observation is necessary to achieve an acceptable performance of the model.

Plotting the data without cleaning it produces the following graph, which one may see is problematic.

```{r}
baseplot <- ggplot(data, aes(x=StartingPosition, y=FinishingPosition) ) +
  stat_density_2d(aes(fill = ..level..), geom = "polygon", colour="white") +
  scale_x_continuous(n.breaks = 24) +
  scale_y_continuous(n.breaks = 24)

baseplot
```

```{r}
data_clean <- na.omit(data)
data_clean <- data_clean[which(data_clean$FinishingPosition != 24),]
```

One can also check the correlation coefficients between all available positions on the cleaned data

```{r}
cor(data_clean[, 10:13])
```

After cleaning, plotting the same data gives a much clearer picture of the relation of the finishing position to the starting position.

```{r}
baseplot <- ggplot(data_clean, aes(x=StartingPosition, y=FinishingPosition) ) +
  stat_density_2d(aes(fill = ..level..), geom = "polygon", colour="white") +
  scale_x_continuous(n.breaks = 24) +
  scale_y_continuous(n.breaks = 24)

baseplot
```

From the correlation matrix and the density scatter plot above one may deduce a linear relationship between the finishing position and the starting position. The next plot includes the linear regression line on the plot.

```{r}
fitPos <- lm(FinishingPosition ~ StartingPosition, data = data_clean)
coeff <- coefficients(fitPos)
baseplot + geom_smooth(method = "lm", se = FALSE, colour="red")

coeff
```

To comment on the results of this regression, we can notice a very strong trend. There is a very high density clustering around the first few starting and finishing position. With further starting positions, the density of points decreases. This could imply that the drivers that get a good starting position have enough of an advantage to pretty surely secure a good finishing position, while it also matters less with worse starting positions.

Next, we also plot the regression of the two practice positions to the finishing position. While these observations are independent from the normal starting position, they show a high linear correlation, which was found to be interesting.

```{r}
baseplotTrain1 <- ggplot(data_clean, aes(x=PositionPractice1, y=FinishingPosition) ) +
  stat_density_2d(aes(fill = ..level..), geom = "polygon", colour="white") +
  scale_x_continuous(n.breaks = 25) +
  scale_y_continuous(n.breaks = 25)
fitPosTrain1 <- lm(FinishingPosition ~ PositionPractice1, data = data_clean)
coeffTrain1 <- coefficients(fitPosTrain1)
baseplotTrain1 + geom_smooth(method = "lm", se = FALSE, colour="red")

coeffTrain1
```

```{r}
baseplotTrain2 <- ggplot(data_clean, aes(x=PositionPractice2, y=FinishingPosition) ) +
  stat_density_2d(aes(fill = ..level..), geom = "polygon", colour="white") +
  scale_x_continuous(n.breaks = 25) +
  scale_y_continuous(n.breaks = 25)
fitPosTrain2 <- lm(FinishingPosition ~ PositionPractice2, data = data_clean)
coeffTrain2 <- coefficients(fitPosTrain2)
baseplotTrain2 + geom_smooth(method = "lm", se = FALSE, colour="red")

coeffTrain2
```

Next, a multiple linear regression was created with the two practice starting positions and the normal starting position. The R\^2 of this regression was slightly better than the one with just the starting position.

```{r}
summary(fitPos)


fitAll <- lm(FinishingPosition ~ StartingPosition + PositionPractice1 + PositionPractice2, data = data_clean)

fitAll
summary(fitAll)

#the R-squared is insignificantly higher than with the model with just the Starting position
```

Next, a step fit was conducted and it concluded that all using all three starting positions produces the lowest possible AIC.

```{r}
fitStep <- step(fitAll, direction = "both")
```

As the final step, the out-of-sample performance was checked among our regression models. The sample was split into 90% training data and 10% test data.

```{r}
n <- nrow(data_clean)
n1 <- floor(n*0.9) # number of obs in train set
set.seed(1234) ## for reproducibility
id_train <- sample(1:n, n1)
train_dat <- data_clean[id_train, ]
test_dat  <- data_clean[-id_train, ]
```

```{r}
fitPos <- lm(FinishingPosition ~ StartingPosition, data = train_dat)
fitAll <- lm(FinishingPosition ~ StartingPosition + PositionPractice1 + 
               PositionPractice2, data = train_dat)

y_hat_Pos <- predict(fitPos, newdata = test_dat) # predictions

y_hat_All <- predict(fitAll, newdata = test_dat) # predictions
```

```{r}
sqrt(c(mean((y_hat_Pos - test_dat$FinishingPosition)^2), 
       mean((y_hat_All - test_dat$FinishingPosition)^2)))
```

The MSE of the model with the three starting positions was lower as well. This should mean that the multiple linear regression model is better at explaining the the finishing position.
